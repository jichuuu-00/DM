{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td>\n",
      "<figure class=\"figure_frm origin_fig\">\n",
      "<p class=\"link_figure\"><img class=\"thumb_g_article\" data-org-src=\"https://t1.daumcdn.net/news/202201/11/segye/20220111190138972egqb.jpg\" data-org-width=\"512\" dmcf-mid=\"EuDMnFEvZl\" dmcf-mtype=\"image\" height=\"auto\" src=\"https://img3.daumcdn.net/thumb/R658x0.q70/?fname=https://t1.daumcdn.net/news/202201/11/segye/20220111190138972egqb.jpg\" width=\"658\"/></p>\n",
      "</figure> </td>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://news.v.daum.net/v/20220111190138086')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "mydata = soup.find('td')\n",
    "print(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\">[1]What is Web Scraping?</h1>\n",
      "[1]What is Web Scraping?\n",
      "[1]What is Web Scraping?\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = \"<html>\\\n",
    "            <body>\\\n",
    "                <h1 id='title'>[1]What is Web Scraping?</h1>\\\n",
    "                <p class='cssstyle'>Web scraping is data scraping used for extracting data from websites</p>\\\n",
    "                <p id='body' align='center'>You need to follow these basic steps:</p>\\\n",
    "            </body>\\\n",
    "        </html>\"\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "data=soup.find('h1')\n",
    "print(data)\n",
    "print(data.get_text())\n",
    "print(data.string) #2nd option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attribute information\n",
    "* data = soup.find('p', class_='cssstyle')\n",
    "* data = soup.find('p', attrs={'class':'cssstyle'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraping is data scraping used for extracting data from websites\n",
      "You need to follow these basic steps:\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = \"<html>\\\n",
    "            <body>\\\n",
    "                <h1 id='title'>[1]What is Web Scraping?</h1>\\\n",
    "                <p class='cssstyle'>Web scraping is data scraping used for extracting data from websites</p>\\\n",
    "                <p id='body' align='center'>You need to follow these basic steps:</p>\\\n",
    "            </body>\\\n",
    "        </html>\"\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "data1=soup.find('p', class_='cssstyle') #for class ONLY\n",
    "data2=soup.find('p', attrs={'id':'body', 'align':'center'})\n",
    "print(data1.get_text())\n",
    "print(data2.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other option\n",
    "* for 'class' ONLY, you can put only value\n",
    "* for 'id', id="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraping is data scraping used for extracting data from websites\n",
      "You need to follow these basic steps:\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = \"<html>\\\n",
    "            <body>\\\n",
    "                <h1 id='title'>[1]What is Web Scraping?</h1>\\\n",
    "                <p class='cssstyle'>Web scraping is data scraping used for extracting data from websites</p>\\\n",
    "                <p id='body' align='center'>You need to follow these basic steps:</p>\\\n",
    "            </body>\\\n",
    "        </html>\"\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "data1=soup.find('p', 'cssstyle') # for class ONLY\n",
    "data2=soup.find('p', id='body') # short version for id ONLY\n",
    "print(data1.get_text())\n",
    "print(data2.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other option 2.\n",
    "* when 2 or more share the same tag\n",
    "* loop statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraping is data scraping used for extracting data from websites\n",
      "You need to follow these basic steps:\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = \"<html>\\\n",
    "            <body>\\\n",
    "                <h1 id='title'>[1]What is Web Scraping?</h1>\\\n",
    "                <p class='cssstyle'>Web scraping is data scraping used for extracting data from websites</p>\\\n",
    "                <p id='body' align='center'>You need to follow these basic steps:</p>\\\n",
    "            </body>\\\n",
    "        </html>\"\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "data_p =soup.find_all('p')\n",
    "for data in data_p:\n",
    "    print(data.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'세계일보 주요 뉴스'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://news.v.daum.net/v/20220111190138086')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "mydata = soup.find('h3', class_='tit_cp')\n",
    "mydata.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise. get reporter information\n",
    "* i.e.. reporter: 윤지로\n",
    "* #span class=\"txt_info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "윤지로\n",
      "입력 2022. 01. 11. 19:01\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://news.v.daum.net/v/20220111190138086')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "mydata = soup.find_all('span', class_='txt_info')\n",
    "for item in mydata:\n",
    "    print(item.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
